# =============================================================================
# UltimaRAG ‚Äî Calibrated Environment Configuration
# =============================================================================
# Hardware: Laptop (Partial CPU Offloading detected for 12B model)
# Status: Calibrated via Benchmark (Stable context ceiling: 2048)
# =============================================================================

# =============================================================================
# üóÉÔ∏è DATABASE CONFIGURATION
# =============================================================================
DB_TYPE=sqlite
SQLITE_DB_PATH=data/ultimarag.db

# =============================================================================
# ü§ñ MODEL CONFIGURATION
# =============================================================================
LLM_PROVIDER=ollama
LLM_BASE_URL=http://localhost:11434
MODEL_NAME=gemma3:4b

# Agent Specialized Models (6GB VRAM Hybrid Setup)
LIGHTWEIGHT_MODEL=qwen3:4b
HEAVY_MODEL=qwen3:8b
AGENT_MODEL_HEALER=deepseek-r1:8b
AGENT_MODEL_FACT_CHECKER=qwen3:8b
AGENT_MODEL_INTENT=qwen3:4b
AGENT_MODEL_HUMANIZER=gemma3:4b

# =============================================================================
# üß† CONTEXT WINDOW & PERFORMANCE (Calibrated)
# =============================================================================
# MASTER Ceiling: Setting MAX_INPUT_TOKENS to 32768 for Gemma3:4b.
# Lowering MAX_OUTPUT_TOKENS to 1024 leaves room for prompt/history/chunks.
MAX_INPUT_TOKENS=32768
MAX_OUTPUT_TOKENS=4096

# Per-task generation caps
LIGHTWEIGHT_MAX_TOKENS=8192
HEAVY_MAX_TOKENS=2048

# Timeouts & Quality
LLM_TIMEOUT=600
LLM_TEMPERATURE=0.0

# =============================================================================
# üß© MEMGPT & RETRIEVAL SOTA SETTINGS
# =============================================================================
# MemGPT auto-scales: Threshold = 0.60 * 32768 = ~19660 tokens for history.
MEMGPT_OVERFLOW_RATIO=0.60
MEMGPT_SUMMARY_TOKENS=256
MEMGPT_CONTENT_TRUNCATE=1500

# Chunking: SOTA standard for 32768 ctx window
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVAL_FINAL_TOP_K=15

# =============================================================================
# üîç EMBEDDINGS & RERANKING
# =============================================================================
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384
RETRIEVAL_DENSE_TOP_K=50
RETRIEVAL_HYBRID_ALPHA=0.7
RETRIEVAL_FINAL_TOP_K=10
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANKER_THRESHOLD=0.1

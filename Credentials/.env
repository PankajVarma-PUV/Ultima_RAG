# =============================================================================
# UltimaRAG ‚Äî Calibrated Environment Configuration
# =============================================================================
# Hardware: Laptop (Partial CPU Offloading detected for 12B model)
# Status: Calibrated via Benchmark (Stable context ceiling: 2048)
# =============================================================================

# =============================================================================
# üóÉÔ∏è DATABASE CONFIGURATION
# =============================================================================
DB_TYPE=sqlite
SQLITE_DB_PATH=data/ultimarag.db

# =============================================================================
# ü§ñ MODEL CONFIGURATION
# =============================================================================
LLM_PROVIDER=ollama
LLM_BASE_URL=http://localhost:11434
MODEL_NAME=gemma3:12b

# =============================================================================
# üß† CONTEXT WINDOW & PERFORMANCE (Calibrated)
# =============================================================================
# MASTER Ceiling: Setting MAX_INPUT_TOKENS to 2048 ensures 100% stability.
# Lowering MAX_OUTPUT_TOKENS to 512 leaves room for prompt/history/chunks.
MAX_INPUT_TOKENS=3072
MAX_OUTPUT_TOKENS=1024

# Per-task generation caps
LIGHTWEIGHT_MAX_TOKENS=512
HEAVY_MAX_TOKENS=1024

# Timeouts & Quality
LLM_TIMEOUT=600
LLM_TEMPERATURE=0.0

# =============================================================================
# üß© MEMGPT & RETRIEVAL SOTA SETTINGS
# =============================================================================
# MemGPT auto-scales: Threshold = 0.80 * 2048 = 1638 tokens for history.
MEMGPT_OVERFLOW_RATIO=0.80
MEMGPT_SUMMARY_TOKENS=128
MEMGPT_CONTENT_TRUNCATE=400

# Chunking: SOTA standard for 2048 ctx window
CHUNK_SIZE=500
CHUNK_OVERLAP=100
RETRIEVAL_FINAL_TOP_K=3

# =============================================================================
# üîç EMBEDDINGS & RERANKING
# =============================================================================
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384
RETRIEVAL_DENSE_TOP_K=50
RETRIEVAL_HYBRID_ALPHA=0.7
RETRIEVAL_FINAL_TOP_K=3
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANKER_THRESHOLD=0.1
